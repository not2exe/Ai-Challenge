name: AI PR Review

on:
  pull_request:
    types: [opened, synchronize]

permissions:
  contents: read
  pull-requests: write

jobs:
  review:
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version-file: go.mod

      - name: Build review agent and mcp-codeindex
        run: |
          go build -o review ./cmd/review
          go build -o mcp-codeindex ./cmd/mcp-codeindex

      - name: Install Ollama
        run: curl -fsSL https://ollama.ai/install.sh | sh

      # Cache the model in user-space so it persists between runs.
      # install.sh creates a system service with models in /usr/share/ollama,
      # but we override OLLAMA_MODELS to use a cacheable path.
      - name: Cache Ollama model
        id: cache-model
        uses: actions/cache@v4
        with:
          path: /home/runner/.ollama-models
          key: ollama-nomic-embed-text-v1

      - name: Start Ollama and pull model
        env:
          OLLAMA_MODELS: /home/runner/.ollama-models
        run: |
          # Stop system service if running, we'll run manually with custom OLLAMA_MODELS
          sudo systemctl stop ollama 2>/dev/null || true

          OLLAMA_MODELS=/home/runner/.ollama-models ollama serve &
          OLLAMA_PID=$!

          # Wait for Ollama to be ready
          for i in $(seq 1 30); do
            if curl -s http://localhost:11434/api/tags > /dev/null 2>&1; then
              echo "Ollama is ready (attempt $i)"
              break
            fi
            sleep 1
          done

          if [ "${{ steps.cache-model.outputs.cache-hit }}" = "true" ]; then
            echo "Model restored from cache, verifying..."
            ollama list
          else
            echo "Pulling nomic-embed-text model..."
            ollama pull nomic-embed-text
          fi

          # Keep Ollama running in background
          echo "OLLAMA_PID=$OLLAMA_PID" >> $GITHUB_ENV

      - name: Run AI Review
        env:
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          OLLAMA_URL: http://localhost:11434
          OLLAMA_MODELS: /home/runner/.ollama-models
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          ./review \
            --pr ${{ github.event.pull_request.number }} \
            --codeindex ./mcp-codeindex \
            --output /tmp/review_result.md

      - name: Post review comment
        if: success()
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          gh pr comment ${{ github.event.pull_request.number }} \
            --body-file /tmp/review_result.md
